# ğŸ§¬ åˆ†å­æ€§è´¨é¢„æµ‹ï¼š30+ç§æœºå™¨å­¦ä¹ å›å½’ç®—æ³•è¯¦è§£

> **å¯¼è¯»**ï¼šåœ¨åˆ†å­æ€§è´¨é¢„æµ‹ã€è¯ç‰©ç­›é€‰ã€ææ–™è®¾è®¡ç­‰å›å½’ä»»åŠ¡ä¸­ï¼Œé€‰å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»30ä½™ç§ç»å…¸å’Œå‰æ²¿çš„å›å½’ç®—æ³•ï¼Œä»ç®€å•çš„çº¿æ€§å›å½’åˆ°å¤æ‚çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼Œå‰–ææ¯ä¸ªæ¨¡å‹çš„åŸç†ã€å…¬å¼å’Œé€‚ç”¨åœºæ™¯ã€‚æ‰€æœ‰æ¨¡å‹å‡ä¸ºscikit-learnä¸­çš„å›å½’å™¨ï¼ˆregressorï¼‰å®ç°ã€‚

## ğŸ“– ç›®å½•

1. [çº¿æ€§æ¨¡å‹å®¶æ—](#1-çº¿æ€§æ¨¡å‹å®¶æ—)
   - 1.1 [æ ¸å¿ƒæ€æƒ³](#11-æ ¸å¿ƒæ€æƒ³)
   - 1.2 [æ¨¡å‹è¯¦è§£](#12-æ¨¡å‹è¯¦è§£)
   - 1.3 [é²æ£’å›å½’å®¶æ—](#13-é²æ£’å›å½’å®¶æ—)
   - 1.4 [å¹¿ä¹‰çº¿æ€§æ¨¡å‹å®¶æ—](#14-å¹¿ä¹‰çº¿æ€§æ¨¡å‹å®¶æ—)
   - 1.5 [çº¿æ€§æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”](#15-çº¿æ€§æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”)
2. [æ”¯æŒå‘é‡æœº](#2-æ”¯æŒå‘é‡æœº)
3. [è¿‘é‚»æ–¹æ³•](#3-è¿‘é‚»æ–¹æ³•)
4. [å†³ç­–æ ‘ä¸éšæœºæ£®æ—](#4-å†³ç­–æ ‘ä¸éšæœºæ£®æ—)
   - 4.1 [å†³ç­–æ ‘å›å½’å™¨](#41-å†³ç­–æ ‘å›å½’å™¨ï¼‰
   - 4.2 [éšæœºæ£®æ—å›å½’å™¨](#42-éšæœºæ£®æ—å›å½’å™¨)
   - 4.3 [æç«¯éšæœºæ ‘å›å½’å™¨](#43-æç«¯éšæœºæ ‘å›å½’å™¨)
   - 4.4 [å†³ç­–æ ‘ä¸éšæœºæ£®æ—å®¶æ—ç»¼åˆå¯¹æ¯”](#44-å†³ç­–æ ‘ä¸éšæœºæ£®æ—å®¶æ—ç»¼åˆå¯¹æ¯”)
5. [æ¢¯åº¦æå‡å®¶æ—](#5-æ¢¯åº¦æå‡å®¶æ—)
   - 5.1 [æ ¸å¿ƒæ€æƒ³](#51-æ ¸å¿ƒæ€æƒ³)
   - 5.2 [æ ‡å‡†æ¢¯åº¦æå‡å›å½’å™¨](#52-æ ‡å‡†æ¢¯åº¦æå‡å›å½’å™¨)
   - 5.3 [æç«¯æ¢¯åº¦æå‡å›å½’å™¨](#53-æç«¯æ¢¯åº¦æå‡å›å½’å™¨)
   - 5.4 [è½»é‡çº§æ¢¯åº¦æå‡å›å½’å™¨](#54-è½»é‡çº§æ¢¯åº¦æå‡å›å½’å™¨)
   - 5.5 [ç±»åˆ«æå‡å›å½’å™¨](#55-ç±»åˆ«æå‡å›å½’å™¨)
   - 5.6 [ç›´æ–¹å›¾æ¢¯åº¦æå‡å›å½’å™¨](#56-ç›´æ–¹å›¾æ¢¯åº¦æå‡å›å½’å™¨)
   - 5.7 [è‡ªé€‚åº”æå‡å›å½’å™¨](#57-è‡ªé€‚åº”æå‡å›å½’å™¨)
   - 5.8 [æ¢¯åº¦æå‡å®¶æ—ç»¼åˆå¯¹æ¯”](#58-æ¢¯åº¦æå‡å®¶æ—ç»¼åˆå¯¹æ¯”)
6. [ç¥ç»ç½‘ç»œ](#6-ç¥ç»ç½‘ç»œ)
   - 6.1 [å¤šå±‚æ„ŸçŸ¥æœºå›å½’å™¨](#61-å¤šå±‚æ„ŸçŸ¥æœºå›å½’å™¨)
7. [æ¦‚ç‡æ¨¡å‹](#7-æ¦‚ç‡æ¨¡å‹)
   - 7.1 [è´å¶æ–¯å²­å›å½’å™¨](#71-è´å¶æ–¯å²­å›å½’å™¨)
   - 7.2 [é«˜æ–¯è¿‡ç¨‹å›å½’å™¨](#72-é«˜æ–¯è¿‡ç¨‹å›å½’å™¨)
   - 7.3 [è‡ªåŠ¨ç›¸å…³æ€§åˆ¤å®šå›å½’å™¨](#73-è‡ªåŠ¨ç›¸å…³æ€§åˆ¤å®šå›å½’å™¨)
   - 7.4 [æ¦‚ç‡æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”](#74-æ¦‚ç‡æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”)
8. [æ·±åº¦ç”Ÿæˆæ¨¡å‹](#8-æ·±åº¦ç”Ÿæˆæ¨¡å‹)
9. [æ¨¡å‹é€‰æ‹©æŒ‡å—](#9-æ¨¡å‹é€‰æ‹©æŒ‡å—)

---

## 1. çº¿æ€§æ¨¡å‹å®¶æ—

### 1.1 æ ¸å¿ƒæ€æƒ³ {#11-æ ¸å¿ƒæ€æƒ³}

çº¿æ€§æ¨¡å‹å‡è®¾è¾“å…¥ç‰¹å¾ä¸ç›®æ ‡å€¼ä¹‹é—´å­˜åœ¨**çº¿æ€§å…³ç³»**ï¼Œé€šè¿‡å­¦ä¹ ç‰¹å¾æƒé‡æ¥è¿›è¡Œé¢„æµ‹ã€‚è¿™æ˜¯æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€å¯è§£é‡Šçš„æ¨¡å‹ç±»å‹ã€‚

### 1.2 æ¨¡å‹è¯¦è§£ {#12-æ¨¡å‹è¯¦è§£}

#### **LinearRegressionï¼ˆçº¿æ€§å›å½’å™¨ï¼‰**

**åŸç†**ï¼šæœ€å°åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³æ–¹è¯¯å·®ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import LinearRegression`

**æ•°å­¦å…¬å¼**ï¼š
$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n = \mathbf{w}^T\mathbf{x}
$$

$$
\min_{\mathbf{w}} \sum_{i=1}^{m} (y_i - \mathbf{w}^T\mathbf{x}_i)^2
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **å¿«é€Ÿè®­ç»ƒ**ï¼šè§£æè§£ï¼Œæ— éœ€è¿­ä»£
- âœ… **é«˜åº¦å¯è§£é‡Š**ï¼šæ¯ä¸ªç‰¹å¾çš„æƒé‡æ¸…æ™°å¯è§
- âŒ **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šé«˜ç»´æ•°æ®æ—¶æƒé‡ä¸ç¨³å®š
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šåˆ†å­æ€§è´¨é¢„æµ‹çš„baselineæ¨¡å‹

---

#### **Ridgeï¼ˆå²­å›å½’å™¨ï¼‰**

**åŸç†**ï¼šåœ¨çº¿æ€§å›å½’åŸºç¡€ä¸ŠåŠ å…¥**L2æ­£åˆ™åŒ–**ï¼Œé˜²æ­¢æƒé‡è¿‡å¤§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import Ridge`

**æ•°å­¦å…¬å¼**ï¼š
$$
\min_{\mathbf{w}} \sum_{i=1}^{m} (y_i - \mathbf{w}^T\mathbf{x}_i)^2 + \alpha \|\mathbf{w}\|_2^2
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **ç¼“è§£å…±çº¿æ€§**ï¼šç›¸å…³ç‰¹å¾çš„æƒé‡æ›´ç¨³å®š
- âœ… **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šæ­£åˆ™åŒ–å‚æ•° $\alpha$ æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šç‰¹å¾æ•°é‡æ¥è¿‘æˆ–è¶…è¿‡æ ·æœ¬æ•°é‡çš„é«˜ç»´åˆ†å­æ•°æ®

---

#### **Lassoï¼ˆå¥—ç´¢å›å½’å™¨ï¼‰**

**åŸç†**ï¼šä½¿ç”¨**L1æ­£åˆ™åŒ–**ï¼Œå¯å°†éƒ¨åˆ†ç‰¹å¾æƒé‡å‹ç¼©ä¸º0ï¼Œå®ç°**ç‰¹å¾é€‰æ‹©**ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import Lasso`

**æ•°å­¦å…¬å¼**ï¼š
$$
\min_{\mathbf{w}} \sum_{i=1}^{m} (y_i - \mathbf{w}^T\mathbf{x}_i)^2 + \alpha \|\mathbf{w}\|_1
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©**ï¼šä¸é‡è¦çš„ç‰¹å¾æƒé‡ä¸º0
- âœ… **ç¨€ç–è§£**ï¼šç»“æœæ›´ç®€æ´æ˜“æ‡‚
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šéœ€è¦è¯†åˆ«å…³é”®åˆ†å­æè¿°ç¬¦æ—¶

---

#### **ElasticNetï¼ˆå¼¹æ€§ç½‘ç»œå›å½’å™¨ï¼‰**

**åŸç†**ï¼šç»“åˆL1å’ŒL2æ­£åˆ™åŒ–ï¼Œå¹³è¡¡ä¸¤è€…ä¼˜åŠ¿ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import ElasticNet`

**æ•°å­¦å…¬å¼**ï¼š
$$
\min_{\mathbf{w}} \sum_{i=1}^{m} (y_i - \mathbf{w}^T\mathbf{x}_i)^2 + \alpha \rho \|\mathbf{w}\|_1 + \frac{\alpha(1-\rho)}{2} \|\mathbf{w}\|_2^2
$$

å…¶ä¸­ $\rho$ æ§åˆ¶L1å’ŒL2çš„æ¯”ä¾‹ã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **ç»¼åˆä¼˜åŠ¿**ï¼šæ—¢èƒ½ç‰¹å¾é€‰æ‹©åˆèƒ½å¤„ç†å…±çº¿æ€§
- âœ… **çµæ´»è°ƒèŠ‚**ï¼šé€šè¿‡ $\rho$ è°ƒæ•´L1/L2æƒé‡
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¤æ‚åˆ†å­æ•°æ®é›†çš„é€šç”¨é¦–é€‰å›å½’å™¨

---

#### **SGDRegressorï¼ˆéšæœºæ¢¯åº¦ä¸‹é™å›å½’å™¨ï¼‰**

**åŸç†**ï¼šé€šè¿‡é€æ ·æœ¬æ›´æ–°æƒé‡å®ç°åœ¨çº¿å­¦ä¹ ï¼Œé€‚åˆè¶…å¤§è§„æ¨¡æ•°æ®ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import SGDRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **å†…å­˜é«˜æ•ˆ**ï¼šæ— éœ€ä¸€æ¬¡åŠ è½½æ‰€æœ‰æ•°æ®
- âœ… **æ”¯æŒåœ¨çº¿å­¦ä¹ **ï¼šæ•°æ®æµå¼æ›´æ–°æ¨¡å‹
- âš¡ **å¿«é€Ÿæ”¶æ•›**ï¼šå¤§æ•°æ®é›†è®­ç»ƒé€Ÿåº¦å¿«
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¤§è§„æ¨¡åˆ†å­æ•°æ®åº“çš„å¢é‡å­¦ä¹ 

---

#### **å…¶ä»–çº¿æ€§æ¨¡å‹**

| æ¨¡å‹ | sklearnç±» | æ ¸å¿ƒç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **BayesianRidge** | `BayesianRidge` | æ¦‚ç‡æ¡†æ¶ï¼Œè‡ªåŠ¨ç¡®å®šæ­£åˆ™åŒ–å¼ºåº¦ | å°æ ·æœ¬å›å½’ï¼Œéœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡ |
| **ARD Regression** | `ARDRegression` | è‡ªåŠ¨ç›¸å…³æ€§åˆ¤å®šï¼Œæè‡´ç‰¹å¾é€‰æ‹© | é«˜ç»´ç¨€ç–åˆ†å­æ•°æ® |
| **Huber Regressor** | `HuberRegressor` | å¯¹å¼‚å¸¸å€¼é²æ£’ | å›å½’æ•°æ®åŒ…å«ç¦»ç¾¤ç‚¹ |
| **Theil-Sen Regressor** | `TheilSenRegressor` | åŸºäºä¸­ä½æ•°ï¼Œæå¼ºé²æ£’æ€§ | ä¸¥é‡æ±¡æŸ“çš„å›å½’æ•°æ® |
| **Passive Aggressive** | `PassiveAggressiveRegressor` | åœ¨çº¿å­¦ä¹ ï¼Œå¯¹é”™è¯¯æ ·æœ¬åŠ å¤§æƒ©ç½š | æµå¼å›å½’æ•°æ®ï¼Œå®æ—¶æ›´æ–° |

---

## 1.3 é²æ£’å›å½’å®¶æ—

å½“æ•°æ®ä¸­å­˜åœ¨**å¼‚å¸¸å€¼**æˆ–**é‡å°¾å™ªå£°**æ—¶ï¼Œæ ‡å‡†æœ€å°äºŒä¹˜æ³•ä¼šå¤±æ•ˆã€‚é²æ£’å›å½’æ¨¡å‹é€šè¿‡ç‰¹æ®Šçš„æŸå¤±å‡½æ•°é™ä½å¼‚å¸¸å€¼çš„å½±å“ã€‚

#### **HuberRegressorï¼ˆèƒ¡ä¼¯å›å½’å™¨ï¼‰**

**æŸå¤±å‡½æ•°**ï¼š
$$
L_\delta(r) = \begin{cases}
\frac{1}{2}r^2 & |r| \leq \delta \\
\delta(|r| - \frac{1}{2}\delta) & |r| > \delta
\end{cases}
$$

å°è¯¯å·®ç”¨å¹³æ–¹æŸå¤±ï¼ˆL2ï¼‰ï¼Œå¤§è¯¯å·®ç”¨ç»å¯¹å€¼æŸå¤±ï¼ˆL1ï¼‰ï¼Œå¹³è¡¡æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import HuberRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **å¯¹ä¸­ç­‰å¼‚å¸¸å€¼é²æ£’**ï¼šé€‚åº¦é™ä½ç¦»ç¾¤ç‚¹å½±å“
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š`epsilon`ï¼ˆå¹³æ–¹/çº¿æ€§æŸå¤±è½¬æ¢ç‚¹ï¼Œé»˜è®¤1.35ï¼‰
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šåŒ…å«ä¸­ç­‰å¼‚å¸¸å€¼çš„åˆ†å­æ€§è´¨å›å½’

---

#### **TheilSenRegressorï¼ˆè¥¿å°”æ£®å›å½’å™¨ï¼‰**

**æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºæ ·æœ¬å¯¹æ–œç‡çš„**ä¸­ä½æ•°**ä¼°è®¡ï¼Œå¯¹å¼‚å¸¸å€¼å…·æœ‰æå¼ºé²æ£’æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import TheilSenRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **æå¼ºé²æ£’æ€§**ï¼šå¯å®¹å¿29.3%çš„å¼‚å¸¸å€¼
- âŒ **ä»…é€‚ç”¨ä½ç»´**ï¼šç‰¹å¾æ•° <20
- âŒ **è®¡ç®—å¤æ‚åº¦é«˜**ï¼š$O(n^2)$
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå®éªŒæ•°æ®è´¨é‡å·®ï¼Œç¦»ç¾¤ç‚¹å¤šçš„åˆ†å­æ€§è´¨å›å½’

---

#### **RANSACRegressorï¼ˆéšæœºé‡‡æ ·ä¸€è‡´æ€§å›å½’å™¨ï¼‰**

**æ ¸å¿ƒæ€æƒ³**ï¼š**éšæœºé‡‡æ ·ä¸€è‡´æ€§**ï¼ˆRandom Sample Consensusï¼‰ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£éšæœºé‡‡æ ·æ‰¾åˆ°æœ€ä¼˜å†…ç‚¹é›†ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import RANSACRegressor`

**ç®—æ³•æµç¨‹**ï¼š
1. éšæœºé‡‡æ ·æœ€å°æ ·æœ¬é›†ï¼Œæ‹Ÿåˆæ¨¡å‹
2. è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æ®‹å·®
3. å°†æ®‹å·®å°äºé˜ˆå€¼çš„æ ·æœ¬æ ‡è®°ä¸ºå†…ç‚¹
4. é‡å¤1-3ï¼Œé€‰æ‹©å†…ç‚¹æœ€å¤šçš„æ¨¡å‹
5. ä½¿ç”¨æ‰€æœ‰å†…ç‚¹é‡æ–°æ‹Ÿåˆæœ€ç»ˆæ¨¡å‹

**ç‰¹ç‚¹**ï¼š
- âœ… **æå¼ºé²æ£’æ€§**ï¼šå¯å¤„ç†>50%å¼‚å¸¸å€¼
- âŒ **ä¸ç¡®å®šæ€§é«˜**ï¼šéšæœºç®—æ³•ï¼Œç»“æœæœ‰æ³¢åŠ¨
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `residual_threshold`ï¼šå†…ç‚¹é˜ˆå€¼
  - `max_trials`ï¼šè¿­ä»£æ¬¡æ•°
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šä¸¥é‡æ±¡æŸ“æ•°æ®ï¼Œå¦‚é«˜é€šé‡åˆ†å­ç­›é€‰çš„æ‰¹æ¬¡æ•ˆåº”

---

#### **QuantileRegressorï¼ˆåˆ†ä½æ•°å›å½’å™¨ï¼‰**

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸é¢„æµ‹å‡å€¼ï¼Œè€Œæ˜¯é¢„æµ‹**æ¡ä»¶åˆ†ä½æ•°**ã€‚

**sklearnå®ç°**ï¼š`from sklearn.ensemble import GradientBoostingRegressor`ï¼ˆè®¾ç½®loss='quantile'ï¼‰

**æŸå¤±å‡½æ•°**ï¼ˆ$\tau$-åˆ†ä½æ•°ï¼‰ï¼š
$$
L_\tau(r) = \begin{cases}
\tau r & r \geq 0 \\
(\tau - 1)r & r < 0
\end{cases}
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ**ï¼šä¸­ä½æ•°å›å½’ï¼ˆ$\tau=0.5$ï¼‰ç‰¹åˆ«é²æ£’
- âœ… **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šé€šè¿‡å¤šä¸ªåˆ†ä½æ•°ï¼ˆå¦‚0.1, 0.5, 0.9ï¼‰ç»™å‡ºé¢„æµ‹åŒºé—´
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š`quantile`ï¼ˆç›®æ ‡åˆ†ä½æ•°ï¼Œ0-1ä¹‹é—´ï¼‰
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå…³æ³¨åˆ†å¸ƒå°¾éƒ¨ï¼Œå¦‚æ¯’æ€§é˜ˆå€¼é¢„æµ‹

---

## 1.4 å¹¿ä¹‰çº¿æ€§æ¨¡å‹å®¶æ—

å½“å“åº”å˜é‡ä¸æœä»æ­£æ€åˆ†å¸ƒæ—¶ï¼Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰é€šè¿‡**é“¾æ¥å‡½æ•°**å°†çº¿æ€§é¢„æµ‹å™¨æ˜ å°„åˆ°åˆé€‚çš„åˆ†å¸ƒç©ºé—´ã€‚

#### **PoissonRegressorï¼ˆæ³Šæ¾å›å½’å™¨ï¼‰**

**é€‚ç”¨åœºæ™¯**ï¼š**è®¡æ•°æ•°æ®**ï¼ˆéè´Ÿæ•´æ•°ï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import PoissonRegressor`

**æ¨¡å‹**ï¼š
$$
\log(\mu) = \mathbf{w}^T\mathbf{x}
$$
$$
y \sim \text{Poisson}(\mu)
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **é€‚åˆè®¡æ•°å‹ç›®æ ‡**ï¼šå¦‚åˆ†å­ä¸­ç‰¹å®šåŸºå›¢æ•°é‡
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šé¢„æµ‹è®¡æ•°å‹åˆ†å­å±æ€§

---

#### **GammaRegressorï¼ˆä¼½é©¬å›å½’å™¨ï¼‰**

**é€‚ç”¨åœºæ™¯**ï¼š**æ­£åæ€è¿ç»­æ•°æ®**ï¼ˆå¦‚æº¶è§£åº¦ã€åŠè¡°æœŸï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import GammaRegressor`

**æ¨¡å‹**ï¼š
$$
\log(\mu) = \mathbf{w}^T\mathbf{x}
$$
$$
y \sim \text{Gamma}(\mu, \alpha)
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **é€‚åˆå³åæ•°æ®**ï¼šå¸¸è§äºç‰©ç†åŒ–å­¦æ€§è´¨
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šè¯ä»£åŠ¨åŠ›å­¦å‚æ•°ï¼ˆæ¸…é™¤ç‡ã€åˆ†å¸ƒä½“ç§¯ç­‰ï¼‰

---

#### **TweedieRegressorï¼ˆç‰¹å¨è¿ªå›å½’å™¨ï¼‰**

**é€‚ç”¨åœºæ™¯**ï¼š**æ··åˆåˆ†å¸ƒ**ï¼ˆåŒ…å«é›¶å€¼å’Œæ­£è¿ç»­å€¼ï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import TweedieRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **çµæ´»åˆ†å¸ƒ**ï¼šé€šè¿‡ `power` å‚æ•°è°ƒæ•´åˆ†å¸ƒå½¢æ€
  - `power=0`ï¼šæ­£æ€åˆ†å¸ƒ
  - `power=1`ï¼šæ³Šæ¾åˆ†å¸ƒ
  - `power=2`ï¼šä¼½é©¬åˆ†å¸ƒ
  - `1<power<2`ï¼šå¤åˆæ³Šæ¾-ä¼½é©¬åˆ†å¸ƒ
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šé«˜é€šé‡åˆ†å­ç­›é€‰æ•°æ®

---

### 1.5 çº¿æ€§æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”

| æ¨¡å‹ | sklearnå®ç° | æ ¸å¿ƒä¼˜åŠ¿ | å±€é™æ€§ | è®¡ç®—å¤æ‚åº¦ | é€‚ç”¨æ•°æ®ç±»å‹ | æ¨èåœºæ™¯ |
|------|-------------|---------|-------|-----------|-----------|
| **LinearRegression** | `LinearRegression` | ç®€å•å¿«é€Ÿï¼Œè§£æè§£ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | $O(n^3)$ | è¿ç»­æ•°å€¼ | åŸºå‡†å›å½’æ¨¡å‹ |
| **Ridge** | `Ridge` | ç¼“è§£å…±çº¿æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ | éœ€è¦è°ƒå‚$\alpha$ | $O(n^3)$ | è¿ç»­æ•°å€¼ | é«˜ç»´æ•°æ® |
| **Lasso** | `Lasso` | è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼Œç¨€ç–è§£ | ç‰¹å¾é—´ç›¸å…³æ€§é«˜æ—¶è¡¨ç°å·® | $O(n^3)$ | è¿ç»­æ•°å€¼ | ç‰¹å¾é€‰æ‹© |
| **ElasticNet** | `ElasticNet` | å…¼é¡¾L1/L2ä¼˜åŠ¿ | éœ€è¦è°ƒå‚$\alpha, \rho$ | $O(n^3)$ | è¿ç»­æ•°å€¼ | å¤æ‚æ•°æ®é€šç”¨ |
| **SGDRegressor** | `SGDRegressor` | å†…å­˜é«˜æ•ˆï¼Œæ”¯æŒåœ¨çº¿å­¦ä¹  | éœ€è¦è°ƒå‚å­¦ä¹ ç‡ | $O(n)$ | ä»»æ„ç±»å‹ | å¤§æ•°æ®æµ |
| **BayesianRidge** | `BayesianRidge` | è‡ªåŠ¨æ­£åˆ™åŒ–ï¼Œæä¾›ä¸ç¡®å®šæ€§ | è®¡ç®—è¾ƒæ…¢ | $O(n^3)$ | è¿ç»­æ•°å€¼ | å°æ ·æœ¬ |
| **ARDRegressor** | `ARDRegressor` | æè‡´ç‰¹å¾é€‰æ‹© | ä»…é€‚ç”¨ç¨€ç–æ•°æ® | $O(n^3)$ | è¿ç»­æ•°å€¼ | è¶…é«˜ç»´ç¨€ç– |
| **HuberRegressor** | `HuberRegressor` | å¯¹ä¸­ç­‰å¼‚å¸¸å€¼é²æ£’ | éœ€è¦è°ƒå‚$\epsilon$ | $O(n^3)$ | è¿ç»­æ•°å€¼ | å«ç¦»ç¾¤ç‚¹ |
| **TheilSenRegressor** | `TheilSenRegressor` | æå¼ºé²æ£’æ€§ | ä»…é€‚ç”¨ä½ç»´ï¼Œè®¡ç®—æ…¢ | $O(n^2)$ | è¿ç»­æ•°å€¼ | ä¸¥é‡æ±¡æŸ“æ•°æ® |
| **RANSACRegressor** | `RANSACRegressor` | å¯å¤„ç†>50%å¼‚å¸¸å€¼ | ç»“æœä¸ç¨³å®šï¼Œéšæœºæ€§ | $O(k \cdot n^2)$ | è¿ç»­æ•°å€¼ | ä¸¥é‡æ±¡æŸ“æ•°æ® |
| **QuantileRegressor** | `QuantileRegressor` | é¢„æµ‹åˆ†ä½æ•°ï¼Œä¸æ•æ„Ÿ | è®¡ç®—æ…¢ï¼Œéœ€è°ƒå‚$\tau$ | $O(n \cdot \log(n))$ | è¿ç»­æ•°å€¼ | éœ€è¦é¢„æµ‹åŒºé—´ |
| **PoissonRegressor** | `PoissonRegressor` | é€‚åˆè®¡æ•°æ•°æ® | ä»…é€‚ç”¨éè´Ÿæ•´æ•° | $O(n^3)$ | è®¡æ•°æ•°æ® | åˆ†å­è®¡æ•°å±æ€§ |
| **GammaRegressor** | `GammaRegressor` | é€‚åˆæ­£åæ€æ•°æ® | ä»…é€‚ç”¨æ­£è¿ç»­å€¼ | $O(n^3)$ | æ­£åè¿ç»­ | ç‰©ç†åŒ–å­¦æ€§è´¨ |
| **TweedieRegressor** | `TweedieRegressor` | çµæ´»åˆ†å¸ƒå½¢æ€ | éœ€è¦è°ƒå‚power | $O(n^3)$ | æ··åˆåˆ†å¸ƒ | é«˜é€šé‡ç­›é€‰ |

---

## 2. æ”¯æŒå‘é‡æœº

### 2.1 æ ¸å¿ƒæ€æƒ³

æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰é€šè¿‡**æœ€å¤§é—´éš”å›å½’**æ¥æ‹Ÿåˆæ•°æ®ï¼Œé€šè¿‡æ ¸å‡½æ•°å¯å¤„ç†éçº¿æ€§å›å½’é—®é¢˜ã€‚

### 2.2 æ¨¡å‹è¯¦è§£

#### **SVRï¼ˆæ”¯æŒå‘é‡å›å½’å™¨ï¼‰**

**åŸç†**ï¼šå®¹å¿é¢„æµ‹å€¼åœ¨çœŸå®å€¼ $\pm \epsilon$ èŒƒå›´å†…çš„è¯¯å·®ï¼Œåªæƒ©ç½šè¶…å‡ºæ­¤èŒƒå›´çš„æ ·æœ¬ã€‚

**sklearnå®ç°**ï¼š`from sklearn.svm import SVR`

**æ•°å­¦å…¬å¼**ï¼š
$$
\min_{\mathbf{w}} \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{m}\max(0, |y_i - \mathbf{w}^T\mathbf{x}_i| - \epsilon)
$$

**æ ¸å‡½æ•°æŠ€å·§**ï¼š
- **Linear Kernel**ï¼š$K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T\mathbf{x}_j$
- **RBF Kernel**ï¼š$K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma\|\mathbf{x}_i - \mathbf{x}_j\|^2)$
- **Polynomial Kernel**ï¼š$K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma\mathbf{x}_i^T\mathbf{x}_j + r)^d$

**ç‰¹ç‚¹**ï¼š
- âœ… **å¤„ç†éçº¿æ€§**ï¼šRBFæ ¸å¯æ‹Ÿåˆå¤æ‚å›å½’å…³ç³»
- âœ… **é²æ£’æ€§å¼º**ï¼šå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼ˆ$\epsilon$-insensitiveï¼‰
- âŒ **è®­ç»ƒç¼“æ…¢**ï¼šå¤§æ•°æ®é›† ($>10^4$ æ ·æœ¬) è®¡ç®—æˆæœ¬é«˜
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `C`ï¼šæ­£åˆ™åŒ–å¼ºåº¦ï¼ˆè¶Šå¤§è¶Šæ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼‰
  - `gamma`ï¼šRBFæ ¸çš„å®½åº¦ï¼ˆè¶Šå¤§è¶Šå…³æ³¨è¿‘é‚»æ ·æœ¬ï¼‰
  - `epsilon`ï¼šå®¹å¿è¯¯å·®èŒƒå›´
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¤æ‚éçº¿æ€§åˆ†å­æ€§è´¨é¢„æµ‹

---

## 3. è¿‘é‚»æ–¹æ³•

### 3.1 K-Nearest Neighborsï¼ˆKè¿‘é‚»å›å½’å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šé¢„æµ‹å€¼ç”±è·ç¦»æœ€è¿‘çš„ $k$ ä¸ªæ ·æœ¬çš„å¹³å‡å€¼å†³å®šã€‚

**sklearnå®ç°**ï¼š`from sklearn.neighbors import KNeighborsRegressor`

**æ•°å­¦å…¬å¼**ï¼ˆå›å½’ï¼‰ï¼š
$$
\hat{y} = \frac{1}{k}\sum_{i \in \mathcal{N}_k(\mathbf{x})} y_i
$$

å…¶ä¸­ $\mathcal{N}_k(\mathbf{x})$ æ˜¯è·ç¦» $\mathbf{x}$ æœ€è¿‘çš„ $k$ ä¸ªæ ·æœ¬é›†åˆã€‚

**è·ç¦»åº¦é‡**ï¼š
- **Euclidean Distance**ï¼š$d(\mathbf{x}_i, \mathbf{x}_j) = \|\mathbf{x}_i - \mathbf{x}_j\|_2$
- **Manhattan Distance**ï¼š$d(\mathbf{x}_i, \mathbf{x}_j) = \|\mathbf{x}_i - \mathbf{x}_j\|_1$

**ç‰¹ç‚¹**ï¼š
- âœ… **é›¶è®­ç»ƒæ—¶é—´**ï¼šæƒ°æ€§å­¦ä¹ ï¼Œæ— éœ€è®­ç»ƒè¿‡ç¨‹
- âœ… **å¤©ç„¶å¤„ç†éçº¿æ€§**ï¼šåŸºäºå±€éƒ¨ä¿¡æ¯
- âŒ **é¢„æµ‹ç¼“æ…¢**ï¼šéœ€è¦è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
- âŒ **å¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿ**ï¼šå»ºè®®å…ˆæ ‡å‡†åŒ–
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `n_neighbors`ï¼šè¿‘é‚»æ•°é‡ï¼ˆé€šå¸¸5-15ï¼‰
  - `weights`ï¼š`uniform`ï¼ˆç­‰æƒï¼‰æˆ– `distance`ï¼ˆè·ç¦»åŠ æƒï¼‰
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå°æ•°æ®é›†å¿«é€Ÿbaselineï¼Œåˆ†å­ç›¸ä¼¼æ€§æœç´¢

---

## 4. å†³ç­–æ ‘ä¸éšæœºæ£®æ—

### 4.1 DecisionTreeRegressorï¼ˆå†³ç­–æ ‘å›å½’å™¨ï¼‰ {#41-å†³ç­–æ ‘å›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ä¸€ç³»åˆ—if-elseè§„åˆ™é€’å½’åˆ’åˆ†ç‰¹å¾ç©ºé—´ã€‚

**sklearnå®ç°**ï¼š`from sklearn.tree import DecisionTreeRegressor`

**åˆ†è£‚å‡†åˆ™**ï¼ˆå›å½’ï¼‰ï¼š
$$
\text{MSE} = \frac{1}{N}\sum_{i=1}^{N}(y_i - \bar{y})^2
$$

æ¯æ¬¡é€‰æ‹©ä½¿å¾—å­èŠ‚ç‚¹MSEä¹‹å’Œæœ€å°çš„ç‰¹å¾å’Œé˜ˆå€¼è¿›è¡Œåˆ†è£‚ã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **æé«˜å¯è§£é‡Šæ€§**ï¼šå†³ç­–è·¯å¾„æ¸…æ™°å¯è§†åŒ–
- âœ… **è‡ªåŠ¨ç‰¹å¾äº¤äº’**ï¼šæ— éœ€æ‰‹åŠ¨æ„é€ äº¤å‰é¡¹
- âœ… **å¤„ç†ç¼ºå¤±å€¼**ï¼šéƒ¨åˆ†å®ç°æ”¯æŒ
- âŒ **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šéœ€è¦å‰ªææˆ–é™åˆ¶æ·±åº¦
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `max_depth`ï¼šæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
  - `min_samples_split`ï¼šåˆ†è£‚èŠ‚ç‚¹æ‰€éœ€æœ€å°æ ·æœ¬æ•°
  - `min_samples_leaf`ï¼šå¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šéœ€è¦è§£é‡Šæ€§çš„åˆ†å­æ€§è´¨é¢„æµ‹

---

### 4.2 RandomForestRegressorï¼ˆéšæœºæ£®æ—å›å½’å™¨ï¼‰ {#42-éšæœºæ£®æ—å›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šè®­ç»ƒå¤šæ£µå†³ç­–æ ‘ï¼Œé€šè¿‡**Bagging + ç‰¹å¾éšæœºé‡‡æ ·**é™ä½æ–¹å·®ã€‚

**sklearnå®ç°**ï¼š`from sklearn.ensemble import RandomForestRegressor`

**ç®—æ³•æµç¨‹**ï¼š
1. Bootstrapé‡‡æ ·ï¼šä»è®­ç»ƒé›†ä¸­æœ‰æ”¾å›æŠ½å– $N$ ä¸ªæ ·æœ¬
2. ç‰¹å¾éšæœºï¼šæ¯æ¬¡åˆ†è£‚åªè€ƒè™‘éšæœºé€‰æ‹©çš„ $\sqrt{p}$ ä¸ªç‰¹å¾
3. ç‹¬ç«‹è®­ç»ƒæ¯æ£µæ ‘
4. é¢„æµ‹æ—¶å–æ‰€æœ‰æ ‘çš„å¹³å‡å€¼

**ç‰¹ç‚¹**ï¼š
- âœ… **å¼ºå¤§æ³›åŒ–èƒ½åŠ›**ï¼šé›†æˆå­¦ä¹ å‡å°‘è¿‡æ‹Ÿåˆ
- âœ… **ç‰¹å¾é‡è¦æ€§**ï¼šå¯è‡ªåŠ¨è¯„ä¼°ç‰¹å¾è´¡çŒ®åº¦
- âœ… **é²æ£’æ€§å¼º**ï¼šå¯¹å™ªå£°å’Œå¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- âœ… **å¹¶è¡Œè®­ç»ƒ**ï¼šå„æ£µæ ‘ç‹¬ç«‹ï¼ŒGPUåŠ é€Ÿå‹å¥½
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `n_estimators`ï¼šæ ‘çš„æ•°é‡ï¼ˆé€šå¸¸100-500ï¼‰
  - `max_features`ï¼šåˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°ï¼ˆé»˜è®¤ $\sqrt{p}$ï¼‰
  - `max_depth`ï¼šæ ‘çš„æœ€å¤§æ·±åº¦

ğŸ“Š **æ¨èåœºæ™¯**ï¼šé€šç”¨é¦–é€‰ï¼Œå¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦çš„åˆ†å­æ€§è´¨é¢„æµ‹

---

### 4.3 ExtraTreesRegressorï¼ˆæç«¯éšæœºæ ‘å›å½’å™¨ï¼‰

**ä¸éšæœºæ£®æ—çš„åŒºåˆ«**ï¼š
- ä¸ä½¿ç”¨Bootstrapé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
- åˆ†è£‚é˜ˆå€¼å®Œå…¨éšæœºé€‰æ‹©ï¼ˆè€Œéæœ€ä¼˜é˜ˆå€¼ï¼‰

**sklearnå®ç°**ï¼š`from sklearn.ensemble import ExtraTreesRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **è®­ç»ƒæ›´å¿«**ï¼šçœå»é˜ˆå€¼æœç´¢æ­¥éª¤
- âœ… **æ›´ä½æ–¹å·®**ï¼šæ›´å¼ºçš„éšæœºæ€§
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¤§è§„æ¨¡åˆ†å­æ•°æ®é›†ï¼Œè¿½æ±‚è®­ç»ƒé€Ÿåº¦

---

### 4.4 å†³ç­–æ ‘ä¸éšæœºæ£®æ—å®¶æ—ç»¼åˆå¯¹æ¯”

| æ¨¡å‹ | sklearnå®ç° | æ ¸å¿ƒä¼˜åŠ¿ | å±€é™æ€§ | è®¡ç®—å¤æ‚åº¦ | å†…å­˜ä½¿ç”¨ | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹é€Ÿåº¦ | å¹¶è¡Œèƒ½åŠ› | æ¨èåœºæ™¯ |
|------|-------------|---------|-------|-----------|---------|---------|----------|----------|---------|
| **DecisionTreeRegressor** | `DecisionTreeRegressor` | æé«˜å¯è§£é‡Šæ€§ï¼Œè‡ªåŠ¨ç‰¹å¾äº¤äº’ | å®¹æ˜“è¿‡æ‹Ÿåˆ | $O(n \log n)$ | ä½ | å¿« | å¿« | âŒ | éœ€è¦è§£é‡Šæ€§çš„å›å½’ä»»åŠ¡ |
| **RandomForestRegressor** | `RandomForestRegressor` | å¼ºå¤§æ³›åŒ–ï¼Œç‰¹å¾é‡è¦æ€§ï¼Œé²æ£’ | å†…å­˜å ç”¨å¤§ | $O(M \cdot n \log n)$ | é«˜ | ä¸­ | ä¸­ | âœ… | é€šç”¨é¦–é€‰å›å½’æ¨¡å‹ |
| **ExtraTreesRegressor** | `ExtraTreesRegressor` | è®­ç»ƒå¿«ï¼Œæ–¹å·®ä½ | éšæœºæ€§å¤§ | $O(M \cdot n \log n)$ | ä¸­ | å¿« | ä¸­ | âœ… | å¤§è§„æ¨¡æ•°æ®ï¼Œè¿½æ±‚è®­ç»ƒé€Ÿåº¦ |

**å¯¹æ¯”è¦ç‚¹**ï¼š
- **è®­ç»ƒé€Ÿåº¦**ï¼šExtraTrees > RandomForest > DecisionTree
- **é¢„æµ‹é€Ÿåº¦**ï¼šDecisionTree > RandomForest â‰ˆ ExtraTrees
- **å†…å­˜å ç”¨**ï¼šDecisionTree < ExtraTrees < RandomForest
- **è¿‡æ‹Ÿåˆé£é™©**ï¼šDecisionTree > RandomForest â‰ˆ ExtraTrees
- **ç‰¹å¾é‡è¦æ€§**ï¼šRandomForestå’ŒExtraTreeséƒ½æ”¯æŒï¼ŒDecisionTreeæ— 

---

## 5. æ¢¯åº¦æå‡å®¶æ—

### 5.1 æ ¸å¿ƒæ€æƒ³ {#51-æ ¸å¿ƒæ€æƒ³}

æ¢¯åº¦æå‡ï¼ˆGradient Boostingï¼‰é€šè¿‡**ä¸²è¡Œ**è®­ç»ƒå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼Œæ¯ä¸ªæ–°æ¨¡å‹ä¸“æ³¨äºæ‹Ÿåˆå‰ä¸€ä¸ªæ¨¡å‹çš„æ®‹å·®ï¼ˆæˆ–æ¢¯åº¦ï¼‰ã€‚

### 5.2 GradientBoostingRegressorï¼ˆæ ‡å‡†æ¢¯åº¦æå‡å›å½’å™¨ï¼‰ {#52-æ ‡å‡†æ¢¯åº¦æå‡å›å½’å™¨}

**sklearnå®ç°**ï¼š`from sklearn.ensemble import GradientBoostingRegressor`

**ç®—æ³•æµç¨‹**ï¼š
1. åˆå§‹åŒ– $F_0(\mathbf{x}) = \bar{y}$
2. å¯¹ $m = 1, 2, \ldots, M$ï¼š
   - è®¡ç®—è´Ÿæ¢¯åº¦ï¼ˆä¼ªæ®‹å·®ï¼‰ï¼š$r_{im} = -\frac{\partial L(y_i, F(\mathbf{x}_i))}{\partial F(\mathbf{x}_i)}$
   - è®­ç»ƒå†³ç­–æ ‘ $h_m$ æ‹Ÿåˆ $r_{im}$
   - æ›´æ–°æ¨¡å‹ï¼š$F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x})$

å…¶ä¸­ $\nu$ æ˜¯å­¦ä¹ ç‡ã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **é«˜å‡†ç¡®æ€§**ï¼šé€šå¸¸ä¼˜äºéšæœºæ£®æ—
- âœ… **çµæ´»æŸå¤±å‡½æ•°**ï¼šæ”¯æŒå¤šç§å›å½’ä»»åŠ¡
- âŒ **è®­ç»ƒç¼“æ…¢**ï¼šä¸²è¡Œè®­ç»ƒæ— æ³•å¹¶è¡Œ
- âŒ **æ˜“è¿‡æ‹Ÿåˆ**ï¼šéœ€è¦ç²¾ç»†è°ƒå‚
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆ0.01-0.3ï¼‰
  - `n_estimators`ï¼šè¿­ä»£æ¬¡æ•°
  - `max_depth`ï¼šæ ‘æ·±åº¦ï¼ˆé€šå¸¸3-8ï¼‰

---

### 5.3 XGBoostRegressorï¼ˆæç«¯æ¢¯åº¦æå‡å›å½’å™¨ï¼‰ {#53-æç«¯æ¢¯åº¦æå‡å›å½’å™¨}

**åˆ›æ–°ç‚¹**ï¼š
- **äºŒé˜¶æ³°å‹’å±•å¼€**ï¼šä½¿ç”¨ä¸€é˜¶å’ŒäºŒé˜¶æ¢¯åº¦ä¿¡æ¯
- **æ­£åˆ™åŒ–**ï¼šåœ¨ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥æ ‘å¤æ‚åº¦æƒ©ç½š
- **åˆ—é‡‡æ ·**ï¼šå€Ÿé‰´éšæœºæ£®æ—çš„ç‰¹å¾é‡‡æ ·
- **å·¥ç¨‹ä¼˜åŒ–**ï¼šå¹¶è¡ŒåŒ–ã€ç¼“å­˜ä¼˜åŒ–ã€GPUåŠ é€Ÿ

**sklearnå®ç°**ï¼š`from xgboost import XGBRegressor`

**ç›®æ ‡å‡½æ•°**ï¼š
$$
\mathcal{L} = \sum_{i=1}^{n}l(y_i, \hat{y}_i) + \sum_{k=1}^{K}\Omega(f_k)
$$

å…¶ä¸­ $\Omega(f_k) = \gamma T + \frac{1}{2}\lambda\|\mathbf{w}\|^2$ï¼ˆ$T$ ä¸ºå¶å­èŠ‚ç‚¹æ•°ï¼Œ$\mathbf{w}$ ä¸ºå¶å­æƒé‡ï¼‰ã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **Kaggleç¥å™¨**ï¼šç«èµ›ä¸­æœ€å¸¸ç”¨æ¨¡å‹ä¹‹ä¸€
- âœ… **å¤„ç†ç¼ºå¤±å€¼**ï¼šè‡ªåŠ¨å­¦ä¹ ç¼ºå¤±å€¼çš„æœ€ä¼˜æ–¹å‘
- âœ… **é€Ÿåº¦å¿«**ï¼šé«˜æ•ˆå·¥ç¨‹å®ç°
- âš™ï¸ **ç‹¬ç‰¹å‚æ•°**ï¼š
  - `subsample`ï¼šè¡Œé‡‡æ ·æ¯”ä¾‹
  - `colsample_bytree`ï¼šåˆ—é‡‡æ ·æ¯”ä¾‹
  - `reg_alpha`, `reg_lambda`ï¼šL1/L2æ­£åˆ™åŒ–

ğŸ“Š **æ¨èåœºæ™¯**ï¼šè¿½æ±‚æè‡´æ€§èƒ½çš„åˆ†å­æ€§è´¨é¢„æµ‹

---

### 5.4 LGBMRegressorï¼ˆè½»é‡çº§æ¢¯åº¦æå‡å›å½’å™¨ï¼‰ {#54-è½»é‡çº§æ¢¯åº¦æå‡å›å½’å™¨}

**åˆ›æ–°ç‚¹**ï¼š
- **GOSSï¼ˆGradient-based One-Side Samplingï¼‰**ï¼šä¿ç•™å¤§æ¢¯åº¦æ ·æœ¬ï¼Œéšæœºé‡‡æ ·å°æ¢¯åº¦æ ·æœ¬
- **EFBï¼ˆExclusive Feature Bundlingï¼‰**ï¼šäº’æ–¥ç‰¹å¾æ‰“åŒ…ï¼Œå‡å°‘ç‰¹å¾ç»´åº¦
- **Leaf-wiseç”Ÿé•¿**ï¼šæŒ‰å¶å­èŠ‚ç‚¹æœ€å¤§å¢ç›Šç”Ÿé•¿ï¼ˆè€Œélevel-wiseï¼‰

**sklearnå®ç°**ï¼š`from lightgbm import LGBMRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **è®­ç»ƒæå¿«**ï¼šå¤§æ•°æ®é›†ä¸Šæ¯”XGBoostå¿«5-10å€
- âœ… **å†…å­˜å ç”¨ä½**ï¼šç‰¹å¾æ‰“åŒ…æŠ€æœ¯
- âœ… **é«˜å‡†ç¡®æ€§**ï¼šä¸XGBoostç›¸å½“æˆ–æ›´å¥½
- âš ï¸ **æ˜“è¿‡æ‹Ÿåˆ**ï¼šLeaf-wiseç­–ç•¥åœ¨å°æ•°æ®é›†ä¸Šéœ€è¦è°¨æ…
- âš™ï¸ **ç‹¬ç‰¹å‚æ•°**ï¼š
  - `num_leaves`ï¼šæœ€å¤§å¶å­èŠ‚ç‚¹æ•°ï¼ˆæ ¸å¿ƒå‚æ•°ï¼‰
  - `min_data_in_leaf`ï¼šå¶å­æœ€å°æ ·æœ¬æ•°

ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¤§è§„æ¨¡åˆ†å­æ•°æ®åº“ï¼ˆ>10ä¸‡æ ·æœ¬ï¼‰

---

### 5.5 CatBoostRegressorï¼ˆç±»åˆ«æå‡å›å½’å™¨ï¼‰ {#55-ç±»åˆ«æå‡å›å½’å™¨}

**åˆ›æ–°ç‚¹**ï¼š
- **Ordered Boosting**ï¼šè§£å†³æ¢¯åº¦ä¼°è®¡åå·®é—®é¢˜
- **åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾**ï¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«ç¼–ç 
- **å¯¹ç§°æ ‘**ï¼šå‡å°‘é¢„æµ‹æ—¶é—´

**sklearnå®ç°**ï¼š`from catboost import CatBoostRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **å¼€ç®±å³ç”¨**ï¼šé»˜è®¤å‚æ•°è¡¨ç°ä¼˜å¼‚
- âœ… **é²æ£’æ€§å¼º**ï¼šå¯¹å‚æ•°ä¸æ•æ„Ÿ
- âœ… **å¤„ç†ç±»åˆ«ç‰¹å¾**ï¼šSMILESå­ç»“æ„ç­‰ç±»åˆ«ä¿¡æ¯
- âŒ **è®­ç»ƒç¨æ…¢**ï¼šç›¸æ¯”LightGBM

ğŸ“Š **æ¨èåœºæ™¯**ï¼šæ··åˆç‰¹å¾ï¼ˆè¿ç»­+ç±»åˆ«ï¼‰çš„åˆ†å­æ•°æ®

---

### 5.6 HistGradientBoostingRegressorï¼ˆç›´æ–¹å›¾æ¢¯åº¦æå‡å›å½’å™¨ï¼‰ {#56-ç›´æ–¹å›¾æ¢¯åº¦æå‡å›å½’å™¨}

**sklearnå®ç°**ï¼š`from sklearn.ensemble import HistGradientBoostingRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼**ï¼šæ— éœ€é¢„å¤„ç†
- âœ… **é€Ÿåº¦å¿«**ï¼šåŸºäºç›´æ–¹å›¾çš„åˆ†è£‚
- âœ… **æ— éœ€å®‰è£…é¢å¤–åº“**ï¼šscikit-learnè‡ªå¸¦
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå¿«é€ŸåŸå‹å¼€å‘ï¼Œä¸éœ€è¦é¢å¤–ä¾èµ–çš„å›å½’ä»»åŠ¡

---

### 5.7 AdaBoostRegressorï¼ˆè‡ªé€‚åº”æå‡å›å½’å™¨ï¼‰ {#57-è‡ªé€‚åº”æå‡å›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¯è½®å¢åŠ é”™è¯¯æ ·æœ¬çš„æƒé‡ï¼Œå¼ºè¿«åç»­æ¨¡å‹å…³æ³¨éš¾åˆ†æ ·æœ¬ã€‚

**sklearnå®ç°**ï¼š`from sklearn.ensemble import AdaBoostRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **ç®€å•æœ‰æ•ˆ**ï¼šå†å²æ‚ ä¹…ï¼Œç†è®ºæˆç†Ÿ
- âŒ **å¯¹å™ªå£°æ•æ„Ÿ**ï¼šå¼‚å¸¸å€¼ä¼šè¢«è¿‡åº¦å…³æ³¨
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šæ•°æ®è´¨é‡é«˜çš„å›å½’é—®é¢˜

---

### 5.8 æ¢¯åº¦æå‡å®¶æ—ç»¼åˆå¯¹æ¯” {#58-æ¢¯åº¦æå‡å®¶æ—ç»¼åˆå¯¹æ¯”}

| æ¨¡å‹ | sklearnå®ç° | æ ¸å¿ƒä¼˜åŠ¿ | è®­ç»ƒæ–¹å¼ | æ­£åˆ™åŒ– | ç‰¹å¾é‡‡æ · | é€‚ç”¨æ•°æ®è§„æ¨¡ | è®¡ç®—æ•ˆç‡ | æ¨èåœºæ™¯ |
|------|-------------|---------|----------|---------|----------|-----------|----------|
| **GradientBoostingRegressor** | `GradientBoostingRegressor` | ç†è®ºæˆç†Ÿï¼Œçµæ´»æŸå¤±å‡½æ•° | ä¸²è¡Œ | æ—  | âŒ | å°-ä¸­æ•°æ®é›† | ä½ | éœ€è¦ç²¾ç»†è°ƒå‚çš„å›å½’ |
| **XGBRegressor** | `XGBRegressor` | ç«èµ›çº§æ€§èƒ½ï¼Œå·¥ç¨‹ä¼˜åŒ–å¥½ | ä¸²è¡Œ | âœ“ | âœ“ | ä¸­-å¤§æ•°æ®é›† | é«˜ | è¿½æ±‚æè‡´æ€§èƒ½çš„å›å½’ |
| **LGBMRegressor** | `LGBMRegressor` | è®­ç»ƒæå¿«ï¼Œå†…å­˜æ•ˆç‡é«˜ | ä¸²è¡Œ | âœ“ | âœ“ | å¤§-è¶…å¤§æ•°æ®é›† | æé«˜ | å¤§æ•°æ®é›†å›å½’é¦–é€‰ |
| **CatBoostRegressor** | `CatBoostRegressor` | å¼€ç®±å³ç”¨ï¼Œå¤„ç†ç±»åˆ«ç‰¹å¾ | ä¸²è¡Œ | âœ“ | âŒ | å°-ä¸­æ•°æ®é›† | ä¸­ | æ··åˆç‰¹å¾çš„å›å½’ |
| **HistGradientBoostingRegressor** | `HistGradientBoostingRegressor` | åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼ï¼Œsklearnè‡ªå¸¦ | ä¸²è¡Œ | âœ“ | âœ“ | ä¸­-å¤§æ•°æ®é›† | é«˜ | å¿«é€ŸåŸå‹å¼€å‘ |
| **AdaBoostRegressor** | `AdaBoostRegressor` | ç®€å•æœ‰æ•ˆï¼Œå†å²æ‚ ä¹… | ä¸²è¡Œ | âŒ | âŒ | å°æ•°æ®é›† | ä½ | æ•°æ®è´¨é‡é«˜çš„å›å½’ |

**å¯¹æ¯”è¦ç‚¹**ï¼š
- **è®­ç»ƒé€Ÿåº¦**ï¼šLGBM > HistGB > XGB > CatBoost > GB > AdaBoost
- **å†…å­˜æ•ˆç‡**ï¼šLGBM > HistGB > XGB > GB â‰ˆ CatBoost > AdaBoost
- **å¤§æ•°æ®é€‚åº”æ€§**ï¼šLGBM > XGB > HistGB > CatBoost > GB > AdaBoost
- **å°æ•°æ®è¡¨ç°**ï¼šAdaBoost > CatBoost > GB > XGB â‰ˆ HistGB > LGBM
- **ç±»åˆ«ç‰¹å¾å¤„ç†**ï¼šCatBoost > XGB â‰ˆ LGBM > HistGB > GB > AdaBoost

---

## 6. ç¥ç»ç½‘ç»œ

### 6.1 MLPRegressorï¼ˆå¤šå±‚æ„ŸçŸ¥æœºå›å½’å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡å¤šå±‚éçº¿æ€§å˜æ¢å­¦ä¹ å¤æ‚çš„ç‰¹å¾è¡¨ç¤ºã€‚

**sklearnå®ç°**ï¼š`from sklearn.neural_network import MLPRegressor`

**å‰å‘ä¼ æ’­**ï¼š
$$
\mathbf{h}^{(1)} = \sigma(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)})
$$
$$
\mathbf{h}^{(2)} = \sigma(\mathbf{W}^{(2)}\mathbf{h}^{(1)} + \mathbf{b}^{(2)})
$$
$$
\hat{y} = \mathbf{W}^{(3)}\mathbf{h}^{(2)} + \mathbf{b}^{(3)}
$$

å…¶ä¸­ $\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆReLUã€Tanhç­‰ï¼‰ã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **å¼ºå¤§è¡¨è¾¾èƒ½åŠ›**ï¼šç†è®ºä¸Šå¯æ‹Ÿåˆä»»æ„å‡½æ•°
- âœ… **ç‰¹å¾å­¦ä¹ **ï¼šè‡ªåŠ¨æå–é«˜å±‚ç‰¹å¾
- âŒ **éœ€è¦å¤§é‡æ•°æ®**ï¼šå°æ ·æœ¬æ˜“è¿‡æ‹Ÿåˆ
- âŒ **è°ƒå‚å›°éš¾**ï¼šå­¦ä¹ ç‡ã€éšè—å±‚ç»“æ„ç­‰
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `hidden_layer_sizes`ï¼šéšè—å±‚ç»“æ„ï¼ˆå¦‚ `(128, 64, 32)`ï¼‰
  - `alpha`ï¼šL2æ­£åˆ™åŒ–å¼ºåº¦
  - `learning_rate_init`ï¼šåˆå§‹å­¦ä¹ ç‡

ğŸ“Š **æ¨èåœºæ™¯**ï¼šç‰¹å¾å¤æ‚ã€æ ·æœ¬å……è¶³çš„å¤§è§„æ¨¡åˆ†å­æ€§è´¨é¢„æµ‹

---

## 7. æ¦‚ç‡æ¨¡å‹

### 7.1 BayesianRidgeï¼ˆè´å¶æ–¯å²­å›å½’å™¨ï¼‰ {#71-è´å¶æ–¯å²­å›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æƒé‡è§†ä¸ºéšæœºå˜é‡ï¼Œä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºä¸ç¡®å®šæ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import BayesianRidge`

**è´å¶æ–¯æ¨æ–­**ï¼š
$$
p(\mathbf{w}|\mathcal{D}) \propto p(\mathcal{D}|\mathbf{w})p(\mathbf{w})
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **è‡ªåŠ¨ç¡®å®šæ­£åˆ™åŒ–å¼ºåº¦**ï¼šæ— éœ€æ‰‹åŠ¨è°ƒå‚
- âœ… **æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡**ï¼šé¢„æµ‹å€¼å¸¦ç½®ä¿¡åŒºé—´
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šå°æ ·æœ¬ã€éœ€è¦ç½®ä¿¡åº¦çš„è¯ç‰©æ—©æœŸå›å½’ç ”ç©¶

---

### 7.2 GaussianProcessRegressorï¼ˆé«˜æ–¯è¿‡ç¨‹å›å½’å™¨ï¼‰ {#72-é«˜æ–¯è¿‡ç¨‹å›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†å‡½æ•°æœ¬èº«å»ºæ¨¡ä¸ºé«˜æ–¯è¿‡ç¨‹ï¼Œé€šè¿‡æ ¸å‡½æ•°å®šä¹‰ç‚¹ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.gaussian_process import GaussianProcessRegressor`

**é¢„æµ‹åˆ†å¸ƒ**ï¼ˆåœ¨è§‚æµ‹æ•°æ® $\mathcal{D}$ ä¸‹ï¼‰ï¼š
$$
p(f(\mathbf{x}_*) | \mathcal{D}) = \mathcal{N}(\mu_*, \sigma_*^2)
$$

å…¶ä¸­å‡å€¼å’Œæ–¹å·®ç”±æ ¸å‡½æ•° $k(\mathbf{x}, \mathbf{x}')$ è®¡ç®—å¾—å‡ºã€‚

**ç‰¹ç‚¹**ï¼š
- âœ… **ä¼˜é›…çš„ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šæä¾›å®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒ
- âœ… **å°æ ·æœ¬å‹å¥½**ï¼šæ•°åä¸ªæ ·æœ¬å³å¯å»ºæ¨¡
- âŒ **è®¡ç®—å¤æ‚åº¦é«˜**ï¼š$O(n^3)$ï¼Œæ ·æœ¬æ•° >1000 æ—¶ä¸å¯è¡Œ
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `kernel`ï¼šæ ¸å‡½æ•°ï¼ˆRBFã€MatÃ©rnç­‰ï¼‰
  - `alpha`ï¼šå™ªå£°æ°´å¹³

ğŸ“Š **æ¨èåœºæ™¯**ï¼šé«˜ä»·å€¼å°æ ·æœ¬åˆ†å­æ•°æ®ï¼Œä¸»åŠ¨å­¦ä¹ 

---

### 7.4 æ¦‚ç‡æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯” {#74-æ¦‚ç‡æ¨¡å‹å®¶æ—ç»¼åˆå¯¹æ¯”}

| æ¨¡å‹ | sklearnå®ç° | ä¸ç¡®å®šæ€§é‡åŒ– | æ ¸å¿ƒä¼˜åŠ¿ | è®¡ç®—å¤æ‚åº¦ | é€‚ç”¨æ•°æ®è§„æ¨¡ | æ¨èåœºæ™¯ |
|------|-------------|-------------|---------|-----------|-----------|
| **BayesianRidge** | `BayesianRidge` | âœ“ | è‡ªåŠ¨æ­£åˆ™åŒ–ï¼Œæ— éœ€è°ƒå‚ | $O(n^3)$ | å°-ä¸­ç­‰ | éœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡ |
| **GaussianProcessRegressor** | `GaussianProcessRegressor` | âœ“ | å®Œæ•´é¢„æµ‹åˆ†å¸ƒï¼Œå°æ ·æœ¬å‹å¥½ | $O(n^3)$ | å°æ ·æœ¬(<1000) | é«˜ä»·å€¼å°æ ·æœ¬ |
| **ARDRegressor** | `ARDRegressor` | âœ— | æè‡´ç‰¹å¾é€‰æ‹© | $O(n^3)$ | ä»»æ„å¤§å° | è¶…é«˜ç»´ç¨€ç– |

**å¯¹æ¯”è¦ç‚¹**ï¼š
- **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šåªæœ‰GaussianProcessRegressoræä¾›å®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒ
- **è®¡ç®—å¤æ‚åº¦**ï¼šBayesianRidge < ARDRegressor < GaussianProcessRegressor
- **é€‚ç”¨è§„æ¨¡**ï¼šGaussianProcessRegressorå—é™äºå°æ ·æœ¬ï¼Œå…¶ä»–ä¸¤è€…é€‚ç”¨ä»»æ„è§„æ¨¡
- **ç‰¹å¾é€‰æ‹©èƒ½åŠ›**ï¼šARDRegressor > BayesianRidge > GaussianProcessRegressor

---

### 7.3 ARDRegressorï¼ˆè‡ªåŠ¨ç›¸å…³æ€§åˆ¤å®šå›å½’å™¨ï¼‰ {#73-è‡ªåŠ¨ç›¸å…³æ€§åˆ¤å®šå›å½’å™¨}

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸ºæ¯ä¸ªç‰¹å¾èµ‹äºˆç‹¬ç«‹çš„ç²¾åº¦å‚æ•°ï¼Œè‡ªåŠ¨åˆ¤å®šç‰¹å¾ç›¸å…³æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import ARDRegression`

**ç‰¹ç‚¹**ï¼š
- âœ… **æè‡´ç‰¹å¾é€‰æ‹©**ï¼šä¸ç›¸å…³ç‰¹å¾çš„æƒé‡ç²¾ç¡®ä¸º0
- âœ… **è´å¶æ–¯æ¡†æ¶**ï¼šè‡ªåŠ¨æ­£åˆ™åŒ–
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šè¶…é«˜ç»´ç¨€ç–åˆ†å­æè¿°ç¬¦æ•°æ®

---

## 8. é²æ£’å›å½’

å½“æ•°æ®ä¸­å­˜åœ¨**å¼‚å¸¸å€¼**æˆ–**é‡å°¾å™ªå£°**æ—¶ï¼Œæ ‡å‡†æœ€å°äºŒä¹˜æ³•ä¼šå¤±æ•ˆã€‚é²æ£’å›å½’æ¨¡å‹é€šè¿‡ç‰¹æ®Šçš„æŸå¤±å‡½æ•°é™ä½å¼‚å¸¸å€¼çš„å½±å“ã€‚

### 8.1 HuberRegressorï¼ˆèƒ¡ä¼¯å›å½’å™¨ï¼‰

**æŸå¤±å‡½æ•°**ï¼š
$$
L_\delta(r) = \begin{cases}
\frac{1}{2}r^2 & |r| \leq \delta \\
\delta(|r| - \frac{1}{2}\delta) & |r| > \delta
\end{cases}
$$

å°è¯¯å·®ç”¨å¹³æ–¹æŸå¤±ï¼ˆL2ï¼‰ï¼Œå¤§è¯¯å·®ç”¨ç»å¯¹å€¼æŸå¤±ï¼ˆL1ï¼‰ï¼Œå¹³è¡¡æ•ˆç‡å’Œé²æ£’æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import HuberRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **å¯¹ä¸­ç­‰å¼‚å¸¸å€¼é²æ£’**ï¼šé€‚åº¦é™ä½ç¦»ç¾¤ç‚¹å½±å“
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š`epsilon`ï¼ˆå¹³æ–¹/çº¿æ€§æŸå¤±è½¬æ¢ç‚¹ï¼Œé»˜è®¤1.35ï¼‰
- ğŸ“Š **æ¨èåœºæ™¯**ï¼šåŒ…å«ä¸­ç­‰å¼‚å¸¸å€¼çš„åˆ†å­æ€§è´¨å›å½’

---

### 8.2 TheilSenRegressorï¼ˆè¥¿å°”æ£®å›å½’å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºæ ·æœ¬å¯¹æ–œç‡çš„**ä¸­ä½æ•°**ä¼°è®¡ï¼Œå¯¹å¼‚å¸¸å€¼å…·æœ‰æå¼ºé²æ£’æ€§ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import TheilSenRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **æå¼ºé²æ£’æ€§**ï¼šå¯å®¹å¿29.3%çš„å¼‚å¸¸å€¼
- âŒ **ä»…é€‚ç”¨ä½ç»´**ï¼šç‰¹å¾æ•° <20
- âŒ **è®¡ç®—å¤æ‚åº¦é«˜**ï¼š$O(n^2)$

ğŸ“Š **æ¨èåœºæ™¯**ï¼šå®éªŒæ•°æ®è´¨é‡å·®ï¼Œç¦»ç¾¤ç‚¹å¤šçš„åˆ†å­æ€§è´¨å›å½’

---

### 8.3 RANSACRegressorï¼ˆéšæœºé‡‡æ ·ä¸€è‡´æ€§å›å½’å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼š**éšæœºé‡‡æ ·ä¸€è‡´æ€§**ï¼ˆRandom Sample Consensusï¼‰ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£éšæœºé‡‡æ ·æ‰¾åˆ°æœ€ä¼˜å†…ç‚¹é›†ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import RANSACRegressor`

**ç®—æ³•æµç¨‹**ï¼š
1. éšæœºé‡‡æ ·æœ€å°æ ·æœ¬é›†ï¼Œæ‹Ÿåˆæ¨¡å‹
2. è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æ®‹å·®
3. å°†æ®‹å·®å°äºé˜ˆå€¼çš„æ ·æœ¬æ ‡è®°ä¸ºå†…ç‚¹
4. é‡å¤1-3ï¼Œé€‰æ‹©å†…ç‚¹æœ€å¤šçš„æ¨¡å‹
5. ä½¿ç”¨æ‰€æœ‰å†…ç‚¹é‡æ–°æ‹Ÿåˆæœ€ç»ˆæ¨¡å‹

**ç‰¹ç‚¹**ï¼š
- âœ… **æå¼ºé²æ£’æ€§**ï¼šå¯å¤„ç†>50%å¼‚å¸¸å€¼
- âŒ **ä¸ç¡®å®šæ€§é«˜**ï¼šéšæœºç®—æ³•ï¼Œç»“æœæœ‰æ³¢åŠ¨
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `residual_threshold`ï¼šå†…ç‚¹é˜ˆå€¼
  - `max_trials`ï¼šè¿­ä»£æ¬¡æ•°

ğŸ“Š **æ¨èåœºæ™¯**ï¼šä¸¥é‡æ±¡æŸ“æ•°æ®ï¼Œå¦‚é«˜é€šé‡åˆ†å­ç­›é€‰çš„æ‰¹æ¬¡æ•ˆåº”

---

### 8.4 QuantileRegressorï¼ˆåˆ†ä½æ•°å›å½’å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸é¢„æµ‹å‡å€¼ï¼Œè€Œæ˜¯é¢„æµ‹**æ¡ä»¶åˆ†ä½æ•°**ã€‚

**sklearnå®ç°**ï¼š`from sklearn.ensemble import GradientBoostingRegressor`ï¼ˆè®¾ç½®loss='quantile'ï¼‰

**æŸå¤±å‡½æ•°**ï¼ˆ$\tau$-åˆ†ä½æ•°ï¼‰ï¼š
$$
L_\tau(r) = \begin{cases}
\tau r & r \geq 0 \\
(\tau - 1)r & r < 0
\end{cases}
$$

**ç‰¹ç‚¹**ï¼š
- âœ… **å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ**ï¼šä¸­ä½æ•°å›å½’ï¼ˆ$\tau=0.5$ï¼‰ç‰¹åˆ«é²æ£’
- âœ… **ä¸ç¡®å®šæ€§é‡åŒ–**ï¼šé€šè¿‡å¤šä¸ªåˆ†ä½æ•°ï¼ˆå¦‚0.1, 0.5, 0.9ï¼‰ç»™å‡ºé¢„æµ‹åŒºé—´
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š`quantile`ï¼ˆç›®æ ‡åˆ†ä½æ•°ï¼Œ0-1ä¹‹é—´ï¼‰

ğŸ“Š **æ¨èåœºæ™¯**ï¼šå…³æ³¨åˆ†å¸ƒå°¾éƒ¨ï¼Œå¦‚æ¯’æ€§é˜ˆå€¼é¢„æµ‹

---

## 9. å¹¿ä¹‰çº¿æ€§æ¨¡å‹

å½“å“åº”å˜é‡ä¸æœä»æ­£æ€åˆ†å¸ƒæ—¶ï¼Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰é€šè¿‡**é“¾æ¥å‡½æ•°**å°†çº¿æ€§é¢„æµ‹å™¨æ˜ å°„åˆ°åˆé€‚çš„åˆ†å¸ƒç©ºé—´ã€‚

### 9.1 PoissonRegressorï¼ˆæ³Šæ¾å›å½’å™¨ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š**è®¡æ•°æ•°æ®**ï¼ˆéè´Ÿæ•´æ•°ï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import PoissonRegressor`

**æ¨¡å‹**ï¼š
$$
\log(\mu) = \mathbf{w}^T\mathbf{x}
$$
$$
y \sim \text{Poisson}(\mu)
$$

ğŸ“Š **æ¨èåœºæ™¯**ï¼šé¢„æµ‹è®¡æ•°å‹åˆ†å­å±æ€§

---

### 9.2 GammaRegressorï¼ˆä¼½é©¬å›å½’å™¨ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š**æ­£åæ€è¿ç»­æ•°æ®**ï¼ˆå¦‚æº¶è§£åº¦ã€åŠè¡°æœŸï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import GammaRegressor`

**æ¨¡å‹**ï¼š
$$
\log(\mu) = \mathbf{w}^T\mathbf{x}
$$
$$
y \sim \text{Gamma}(\mu, \alpha)
$$

ğŸ“Š **æ¨èåœºæ™¯**ï¼šè¯ä»£åŠ¨åŠ›å­¦å‚æ•°ï¼ˆæ¸…é™¤ç‡ã€åˆ†å¸ƒä½“ç§¯ç­‰ï¼‰

---

### 9.3 TweedieRegressorï¼ˆç‰¹å¨è¿ªå›å½’å™¨ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š**æ··åˆåˆ†å¸ƒ**ï¼ˆåŒ…å«é›¶å€¼å’Œæ­£è¿ç»­å€¼ï¼‰ã€‚

**sklearnå®ç°**ï¼š`from sklearn.linear_model import TweedieRegressor`

**ç‰¹ç‚¹**ï¼š
- âœ… **çµæ´»åˆ†å¸ƒ**ï¼šé€šè¿‡ `power` å‚æ•°è°ƒæ•´åˆ†å¸ƒå½¢æ€
  - `power=0`ï¼šæ­£æ€åˆ†å¸ƒ
  - `power=1`ï¼šæ³Šæ¾åˆ†å¸ƒ
  - `power=2`ï¼šä¼½é©¬åˆ†å¸ƒ
  - `1<power<2`ï¼šå¤åˆæ³Šæ¾-ä¼½é©¬åˆ†å¸ƒ

ğŸ“Š **æ¨èåœºæ™¯**ï¼šé«˜é€šé‡åˆ†å­ç­›é€‰æ•°æ®

---

## 10. æ·±åº¦ç”Ÿæˆæ¨¡å‹

### 10.1 VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡**ç¼–ç å™¨-è§£ç å™¨**æ¶æ„å­¦ä¹ æ•°æ®çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºï¼ŒåŒæ—¶åˆ©ç”¨**å˜åˆ†æ¨æ–­**ç¡®ä¿æ½œåœ¨ç©ºé—´çš„å¹³æ»‘æ€§ã€‚

**æ¨¡å‹æ¶æ„**ï¼š
$$
\text{Encoder}: \mathbf{x} \rightarrow \mathcal{N}(\mu(\mathbf{x}), \sigma^2(\mathbf{x}))
$$
$$
\text{Latent}: \mathbf{z} \sim \mathcal{N}(\mu, \sigma^2)
$$
$$
\text{Decoder}: \mathbf{z} \rightarrow \hat{\mathbf{x}}
$$

**æŸå¤±å‡½æ•°**ï¼š
$$
\mathcal{L} = \underbrace{\|\mathbf{x} - \hat{\mathbf{x}}\|^2}_{\text{é‡æ„æŸå¤±}} + \beta \cdot \underbrace{D_{KL}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))}_{\text{KLæ•£åº¦æ­£åˆ™åŒ–}}
$$

**å¸¸è§å˜ä½“**ï¼š
- **VAE (latent=64/128/256)**ï¼šä¸åŒæ½œåœ¨ç»´åº¦ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œä¿¡æ¯ä¿ç•™
- **VAE (compact)**ï¼šæµ…å±‚ç½‘ç»œï¼Œå¿«é€Ÿè®­ç»ƒ
- **VAE (deep)**ï¼šæ·±å±‚ç½‘ç»œï¼Œæ›´å¼ºè¡¨è¾¾èƒ½åŠ›

**ç‰¹ç‚¹**ï¼š
- âœ… **æ— ç›‘ç£ç‰¹å¾å­¦ä¹ **ï¼šè‡ªåŠ¨ä»å‘é‡è¡¨ç¤ºæå–æ·±å±‚ç‰¹å¾
- âœ… **é™ç»´èƒ½åŠ›å¼º**ï¼šé«˜ç»´æŒ‡çº¹â†’ä½ç»´æ½œåœ¨å‘é‡
- âœ… **æ”¯æŒç”Ÿæˆ**ï¼šå¯ç”¨äºåˆ†å­ç”Ÿæˆï¼ˆè™½ç„¶ä¸»è¦ç”¨äºå›å½’ï¼‰
- âŒ **è®­ç»ƒå¤æ‚**ï¼šéœ€è¦GPUåŠ é€Ÿï¼Œè°ƒå‚å›°éš¾
- âš™ï¸ **å…³é”®å‚æ•°**ï¼š
  - `latent_dim`ï¼šæ½œåœ¨ç©ºé—´ç»´åº¦
  - `beta`ï¼šKLæ•£åº¦æƒé‡ï¼ˆÎ²-VAEï¼‰

ğŸ“Š **æ¨èåœºæ™¯**ï¼š
- é«˜ç»´ç¨€ç–æ•°æ®
- éœ€è¦ç‰¹å¾é™ç»´çš„è¿ç§»å­¦ä¹ 
- ä¸ä¼ ç»ŸMLæ¨¡å‹é…åˆä½¿ç”¨

---

## 11. æ¨¡å‹é€‰æ‹©æŒ‡å—

### 11.1 æŒ‰åº”ç”¨åœºæ™¯é€‰æ‹©

| åœºæ™¯ | æ¨èæ¨¡å‹ | ç†ç”± |
|------|---------|------|
| **å¿«é€Ÿbaseline** | LinearRegression, Ridge, KNeighborsRegressor | è®­ç»ƒæå¿«ï¼Œè¯„ä¼°å›å½’æ¨¡å‹å¯è¡Œæ€§ |
| **è¿½æ±‚å‡†ç¡®ç‡** | XGBoost, LightGBM, RandomForestRegressor | é›†æˆå­¦ä¹ ï¼Œæ€§èƒ½æœ€ä½³ |
| **å°æ ·æœ¬(<100)** | BayesianRidge, GaussianProcessRegressor | è´å¶æ–¯æ–¹æ³•ï¼Œæä¾›ä¸ç¡®å®šæ€§ |
| **å¤§æ•°æ®é›†(>100k)** | LGBMRegressor, SGDRegressor | å†…å­˜é«˜æ•ˆï¼Œè®­ç»ƒå¿«é€Ÿ |
| **éœ€è¦å¯è§£é‡Šæ€§** | LinearRegression, Ridge, Lasso, DecisionTreeRegressor | æ¸…æ™°çš„ç‰¹å¾æƒé‡æˆ–å†³ç­–è§„åˆ™ |
| **æ•°æ®æœ‰ç¦»ç¾¤ç‚¹** | HuberRegressor, TheilSenRegressor, RANSACRegressor, RandomForestRegressor | é²æ£’æŸå¤±å‡½æ•°æˆ–é›†æˆæ–¹æ³• |
| **è®¡æ•°æ•°æ®** | PoissonRegressor | ç¬¦åˆæ•°æ®åˆ†å¸ƒå‡è®¾ |
| **é«˜ç»´ç¨€ç–æ•°æ®** | Lasso, ElasticNet, ARDRegressor | L1æ­£åˆ™åŒ–ç‰¹å¾é€‰æ‹© |
| **æ·±åº¦ç‰¹å¾å­¦ä¹ ** | VAE, MLPRegressor | éçº¿æ€§è¡¨å¾å­¦ä¹  |
| **ä¸ç¡®å®šæ€§é‡åŒ–** | GaussianProcessRegressor, BayesianRidge, QuantileRegressor | æä¾›ç½®ä¿¡åŒºé—´æˆ–é¢„æµ‹åˆ†å¸ƒ |
| **å¤æ‚éçº¿æ€§** | SVR, XGBoost, MLPRegressor | å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³» |
| **å®æ—¶é¢„æµ‹** | LinearRegression, DecisionTreeRegressor | æ¨ç†é€Ÿåº¦å¿« |

---

### 11.2 æŒ‰æ•°æ®ç‰¹å¾é€‰æ‹©

#### **ç‰¹å¾ç»´åº¦**
- **ä½ç»´(<10)**ï¼šä»»æ„å›å½’æ¨¡å‹
- **ä¸­ç»´(10-100)**ï¼šRandomForestRegressor, GradientBoostingRegressor, Lasso
- **é«˜ç»´(100-10000)**ï¼šLasso, ElasticNet, LGBMRegressor, VAE
- **è¶…é«˜ç»´(>10000)**ï¼šLasso, ARDRegressor, VAE

#### **æ ·æœ¬æ•°é‡**
- **å°æ ·æœ¬(<100)**ï¼šLinearRegression, Ridge, GaussianProcessRegressor
- **ä¸­ç­‰æ ·æœ¬(100-10k)**ï¼šRandomForestRegressor, XGBoost, SVR
- **å¤§æ ·æœ¬(>10k)**ï¼šLGBMRegressor, SGDRegressor, MLPRegressor
- **è¶…å¤§æ ·æœ¬(>100k)**ï¼šLGBMRegressor, SGDRegressor

#### **æ•°æ®è´¨é‡**
- **å™ªå£°å°**ï¼šä»»æ„å›å½’æ¨¡å‹
- **ä¸­ç­‰å™ªå£°**ï¼šRandomForestRegressor, GradientBoostingRegressor
- **å™ªå£°å¤§/æœ‰ç¦»ç¾¤ç‚¹**ï¼šHuberRegressor, TheilSenRegressor, RANSACRegressor, QuantileRegressor

---

### 11.3 æŒ‰è®¡ç®—èµ„æºé€‰æ‹©

| èµ„æºé™åˆ¶ | æ¨èæ¨¡å‹ | é¿å…æ¨¡å‹ |
|---------|---------|---------|
| **å†…å­˜æœ‰é™** | LinearRegression, Ridge, SGDRegressor, LGBMRegressor | RandomForestRegressorï¼ˆn_estimatorså¤§ï¼‰, GaussianProcessRegressor |
| **CPUæœ‰é™** | LinearRegression, Ridge, DecisionTreeRegressor | SVRï¼ˆå¤§æ•°æ®é›†ï¼‰, GradientBoostingRegressor |
| **æœ‰GPU** | MLPRegressor, VAE, XGBoost/LGBMRegressorï¼ˆGPUç‰ˆæœ¬ï¼‰ | - |
| **éœ€è¦å¿«é€Ÿè®­ç»ƒ** | LinearRegression, Ridge, DecisionTreeRegressor, LGBMRegressor | SVR, GaussianProcessRegressor, MLPRegressor |
| **éœ€è¦å¿«é€Ÿé¢„æµ‹** | LinearRegression, Ridge, RandomForestRegressorï¼ˆå°ï¼‰ | KNeighborsRegressor, GaussianProcessRegressor |

---

### 11.4 æ¨¡å‹ç»„åˆç­–ç•¥

**æ¨èçš„å›å½’æ¨¡å‹ç­›é€‰ç­–ç•¥**ï¼š

1. **å¿«é€Ÿæ¢ç´¢é˜¶æ®µ**
   - LinearRegression, Ridge, DecisionTreeRegressor, KNeighborsRegressor, SGDRegressor
   - ç›®çš„ï¼šå¿«é€Ÿè¯„ä¼°æ•°æ®å¯é¢„æµ‹æ€§

2. **å‡†ç¡®æ€§ä¼˜åŒ–é˜¶æ®µ**
   - RandomForestRegressor, GradientBoostingRegressor, XGBoost, LGBMRegressor
   - ç›®çš„ï¼šè·å–æœ€ä½³æ€§èƒ½å›å½’æ¨¡å‹

3. **é²æ£’æ€§éªŒè¯é˜¶æ®µ**
   - HuberRegressor, TheilSenRegressor, RANSACRegressor, RandomForestRegressor
   - ç›®çš„ï¼šè¯„ä¼°å¼‚å¸¸å€¼å¯¹å›å½’æ€§èƒ½çš„å½±å“

4. **å¯è§£é‡Šæ€§åˆ†æé˜¶æ®µ**
   - Lasso, DecisionTreeRegressor, LinearRegression
   - ç›®çš„ï¼šç†è§£å½±å“åˆ†å­æ€§è´¨çš„å…³é”®ç‰¹å¾

5. **æ·±åº¦å­¦ä¹ å¤‡é€‰**
   - VAEç³»åˆ—, MLPRegressor
   - ç›®çš„ï¼šæ¢ç´¢åˆ†å­æ€§è´¨çš„æ·±å±‚éçº¿æ€§ç‰¹å¾

---

## 12. æ€»ç»“

æœ¬æ–‡ä»‹ç»äº†è¦†ç›–ä»ç»å…¸åˆ°å‰æ²¿çš„30+ç§æœºå™¨å­¦ä¹ **å›å½’**æ¨¡å‹ï¼Œå½¢æˆäº†å®Œæ•´çš„**å›å½’ç®—æ³•ç”Ÿæ€**ï¼š

âœ… **çº¿æ€§å›å½’æ¨¡å‹**ï¼šç®€å•å¿«é€Ÿï¼Œé«˜åº¦å¯è§£é‡Š
âœ… **æ”¯æŒå‘é‡å›å½’**ï¼šå¤„ç†éçº¿æ€§ï¼Œå°æ ·æœ¬é«˜ç»´å‹å¥½
âœ… **å†³ç­–æ ‘ä¸æ£®æ—å›å½’å™¨**ï¼šå¼ºå¤§æ³›åŒ–ï¼Œç‰¹å¾é‡è¦æ€§åˆ†æ
âœ… **æ¢¯åº¦æå‡å›å½’å™¨**ï¼šå‡†ç¡®æ€§ä¹‹ç‹ï¼Œç«èµ›é¦–é€‰
âœ… **ç¥ç»ç½‘ç»œå›å½’å™¨**ï¼šæ·±åº¦å­¦ä¹ ï¼Œå¤æ‚æ¨¡å¼æ•æ‰
âœ… **æ¦‚ç‡å›å½’æ¨¡å‹**ï¼šä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè´å¶æ–¯æ¡†æ¶
âœ… **é²æ£’å›å½’å™¨**ï¼šå¯¹æŠ—å¼‚å¸¸å€¼ï¼Œæ•°æ®æ¸…æ´—å›°éš¾æ—¶çš„æ•‘æ˜Ÿ
âœ… **å¹¿ä¹‰çº¿æ€§å›å½’æ¨¡å‹**ï¼šç‰¹æ®Šåˆ†å¸ƒæ•°æ®çš„ä¸“å®¶
âœ… **æ·±åº¦ç”Ÿæˆæ¨¡å‹**ï¼šVAEæä¾›ç‰¹å¾å­¦ä¹ ä¸é™ç»´èƒ½åŠ›

æ— è®ºæ‚¨ä»äº‹è¯ç‰©å‘ç°ã€ææ–™è®¾è®¡è¿˜æ˜¯åˆ†å­æ€§è´¨é¢„æµ‹ï¼Œéƒ½èƒ½æ‰¾åˆ°åˆé€‚çš„**å›å½’å·¥å…·**ã€‚è®°ä½ï¼š**æ²¡æœ‰ä¸‡èƒ½çš„å›å½’å™¨ï¼Œåªæœ‰æœ€é€‚åˆçš„å›å½’å™¨**ã€‚å»ºè®®å…ˆä½¿ç”¨å¿«é€Ÿå›å½’æ¨¡å‹å»ºç«‹baselineï¼Œå†ç”¨é«˜å‡†ç¡®æ€§å›å½’å™¨ä¼˜åŒ–æ€§èƒ½ï¼Œæœ€åé€šè¿‡å¯è§£é‡Šå›å½’å™¨ç†è§£å…³é”®ç‰¹å¾ã€‚

Happy Regression Modeling! ğŸš€

---

## å‚è€ƒèµ„æ–™

1. Scikit-learn Documentation: https://scikit-learn.org/
2. XGBoost Documentation: https://xgboost.readthedocs.io/
3. LightGBM Documentation: https://lightgbm.readthedocs.io/
4. CatBoost Documentation: https://catboost.ai/docs/
5. Kingma & Welling (2013). "Auto-Encoding Variational Bayes"
6. Hastie et al. (2009). "The Elements of Statistical Learning"
7. Bishop (2006). "Pattern Recognition and Machine Learning"
