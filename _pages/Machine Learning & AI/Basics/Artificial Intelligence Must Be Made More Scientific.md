---
title: "人工智能必须更科学：让AI与科学方法兼容"
date: "2025-11-07"
tags: [artificial-intelligence, scientific-method, machine-learning, ethics, reproducibility, transparency, causality]
description: "深入探讨AI与科学方法的兼容性，分析当前AI在可重复性、透明性、客观性和机制理解方面的局限，提出与科学方法完全兼容的AI发展方向"
image: "/assets/img/thumbnail/bricks.webp"
thumbnail: "/assets/img/La-Mancha.jpg"
author: Xufan Gao
lang: zh-CN
---

# 人工智能必须更科学：让AI与科学方法兼容

## 本文信息

* **标题**: 人工智能必须更“科学”：让AI与科学方法兼容
* **作者**: Peter V. Coveney, Roger Highfield
* 发表时间: 2024年7月27日
* **单位**: University College London（英国）；University of Amsterdam（荷兰）；Ludwig Maximilian University of Munich（德国）；Science Museum London（英国）；University of Oxford（英国）
* **引用格式**: Coveney, P. V., & Highfield, R. (2024). Artificial Intelligence Must Be Made More Scientific. *Journal of Chemical Information and Modeling*, *64*(13), 5739–5741. [https://doi.org/10.1021/acs.jcim.4c01091](https://doi.org/10.1021/acs.jcim.4c01091)

## 摘要

> 随着人工智能在科研中的作用不断扩大，作者评估了其对研究实践的影响，指出当前一代AI**缺乏可重复性**、**缺乏透明性**、**缺乏客观性**以及**缺乏机制层面的理解**。文章强调，**科学的核心在于经验与理性的统一**，通过理论与实验的循环推动知识进步；而当下许多AI系统更多停留在**统计拟合与相关性层面**，难以提供可解释的不确定性与因果机制。作者以AlphaFold与机器学习势能（MLIP）为例，比较了基于物理的模型与纯数据驱动方法在不确定性量化与参数可解释性上的差异；讨论了生成式方法与大模型在再现性、数据依赖与人类偏见方面的局限。为使AI真正惠及科学研究，作者主张发展与科学方法**完全兼容**的AI形态，包括**可解释AI**、**因果AI**与与物理定律耦合的Big AI。

### 核心结论

* **AI要服务科学，必须满足可重复性、透明性、客观性与机制解释**

* 单纯基于相关性的**黑箱**模型难以量化不确定性，也难以支撑科学理解

* **与物理约束、可解释机制和因果推断相结合的AI更接近科学方法**

* **科学共同体需要对AI提出更高标准，而非被炒作与功利目标牵引**

---

## 全文翻译

人工智能在科学中的作用与日俱增。我们在此评估其对研究的影响，并指出AI常常缺少**可重复性**、**透明性**、**客观性**与**机制层面的理解**。要确保AI真正造福研究，我们需要发展与**科学方法完全兼容**的AI形式。

人工智能正在深入科学，尽管它距离媒体标题中更离奇的宣称还有很长的路。但它是否改变了我们对科学的理解？答案是一个**明确的不会**。在许多方面，当前一代AI甚至谈不上科学。

关于科学的确切定义，哲学家与科学史家存在分歧，但普遍共识是：**科学是观察与理性的融合**。极端经验主义（只有数据没有理论）与极端理性主义（只有理论没有数据）早在几个世纪前就被摒弃了。取而代之，科学家将理论用于做出预测并引导新实验，通过实验产出数据以塑造理论，周而复始。**可重复性**被赋予极高权重，这保证了科学的**客观性**，也使其区别于其他人类活动。

几百年前，培根用“蜜蜂”比喻科学家如何滋养理性与经验的共生。随着计算机兴起，另一种科学形态兴起：模拟能够给出可操作的预测。将描述我们对大气与海洋理解的数学模型，与来自卫星与地面站的数据结合，就能进行挽救生命的天气预报。面向未来的最具代表性的例子，是人体的数字孪生。

如今我们进入计算的新纪元，AI的重要性不断上升。然而少有人记得此前的炒作与低谷周期。我们也常忘记，人类 **20** 瓦的大脑能力依然惊人，哪怕与耗能高出一百万倍的百亿亿次超级计算机相比亦然。令人尴尬的是，关于“自然智能”的公认定义并不存在，那么我们所谓的“AI”究竟指什么？我们对计算机寄予了过度信任。

尽管有这些问题，美国大型科技公司仍在做出**大胆甚至夸张的宣称**。它们有一个**压倒性的动机：盈利**。大型机构因害怕错过风口而争相拥抱AI。政府也乐于上车，指望AI让其更有效率、更有说服力。

一些最狂热的追随者宣称，计算机算法可以**超越人类智能**，机器能够接管人类的许多职能。具有讽刺意味的是，其中一些最夸张的说法来自那些依赖大规模众包劳工的公司——贝索斯称之为人工的人工智能或伪AI，用来帮助AI完成繁琐却棘手的任务。

人们懒于思考地假设AI也可以做科学。但机器学习方法过去与现在本质上都是**模式发现者**，旨在解决工程技术问题。它们的起源更多与情报与安全部门有关，目标是让计算机从海量数据中筛选线索，而非让科学家理解自然。

在这个领域AI当然能发挥作用。最著名的例子或许是蛋白质结构预测软件AlphaFold，它绘制了几乎所有已知蛋白的“结构宇宙”。对分子生物学家来说，AlphaFold是X射线晶体学的快速替代。和许多机器学习一样，AlphaFold最擅长处理它被训练“见过”的模式。但由于本质上接近“查找表”，我们很难判断它在什么情况下可靠、在什么情况下会失效。换言之，量化它的不确定性很困难。

另一个热门话题是用AI学习相互作用势能（MLIP），以供经典分子动力学仿真。决定这些势函数的形式或参数化是繁琐的，因此有人提出用AI从尽可能大的数据集中学习从原子性质到分子势能或其他量的映射。这会得到一个拥有几十万个拟合参数的神经网络——这些参数是神经元之间的连接权重。同样地，量化这类MLIP的不确定性很难，原因有二：参数数量过多，且这些参数只是拟合参数，没有内在的物理化学含义。

事实上，我们对分子相互作用的科学理解已经很成熟。因而也可以采用**基于物理的相互作用势**，其项具有明确的科学意义，参数数量从数百到几千不等。借助可扩展的不确定性量化方法，人们发现通常只有 **10** 到 **20** 个力场参数对目标性质具有显著影响。换句话说，我们能够获得哪些参数重要的**真实洞见**与理解。

相比之下，我们很难理解MLIP或AlphaFold内部发生了什么。这些系统需要从**几十万到上亿级的参数**。一方面，**天文数量级的参数解释了为何机器学习能够拟合大量任意关系**；另一方面，这也导致其**不可靠**，且**无法给出令人满意的科学解释**。

此外，它们通常在选定的数据集上训练，再以较小的验证集做评估。但换一个数据集，它们是否仍然有效？很多时候并不行，因为此时模型在做**外推而非内插**。

生成式方法存在类似问题，且更依赖随机数发生器，因此更进一步地说，代码每次运行都会给出不同答案。这让人联想到分子动力学：**一次性模拟不可复现**。可重复性还面临其他挑战，包括获取底层数据与机器学习算法的渠道，这些可能被保密，且有时还需要大量算力的支持。

科学追求的是**理解**，而AI依赖的是统计推断。这并非错误本身，但请记住：**相关并不等于因果**。借助遍历性、拉姆齐理论与算法信息论，可以证明：大数据库中会包含任意多的相关性，且相关性的数量随着数据量而快速增加，而非随着数据“本质”的改变而变化。即使在随机生成的超大数据库中也会涌现大量相关性，这意味着大多数相关性是伪的。要从中筛出真正的相关性，需要**科学方法**。

尽管计算机创造了“客观性”的表象，人仍然在AI的建立与使用中居于核心。大多数情况下，为了训练AI，你必须预先定义AI将把答案归入的类别。但任何这种分类都是任意的、歧义丛生，反映开发者自身的动机：**人类偏见被烘焙进AI之中**，在训练之前就已存在。

AI通常建立在一系列也体现人类选择的假设之上，而非源于科学。例如，**几乎所有机器学习算法都假设内部数据分析变量之间的关系是平滑可微的**。这纯粹出于方便，便于使用线性代数、标准软件库以及GPU加速。然而，AI与机器学习确实能产生各种非线性预测。这是因为在以线性代数为主的同时，它们加入了将输入映射到输出的**非线性激活函数**。

如果我们自我安慰地假定世界处处可微，就可能进一步假定：在浮点数表示上从双精度退到半精度乃至四分之一精度牺牲一点精确度无关紧要，或者高斯统计的钟形曲线是无所不能的。在真实世界中，这些假设通常不成立。**尖锐的不连续**广泛存在，这是非线性行为的标志。

归根结底，**世界高度是非线性的**。因为非线性科学直觉上难以把握且往往不可微，人们会倾向于回避它。非线性的极端表现之一是：**舍入误差**会在数字计算机中引发深远影响——这一点常被忽视。

可以理解，为什么一些科学家把AI当作替代培根蜜蜂的方案：在诸如生命科学这样的复杂领域，AI对**答案**的追逐而非对**理解**的追求，的确具有诱惑力。但在医疗等领域，这是**不可接受的**。我们必须理解治疗方案如何起作用，且消除其内在偏见——不仅是训练数据的代表性问题，还包括AI系统在设计之初的偏见。

一些人对新一波基础模型的兴奋在增长。这些通用目的AI被宣传为可以通过类似聊天界面的交互来解决科学家的问题。所谓AI4Science的例子包括用于分子分布的DiG、无机材料设计的MatterGen、以及目标感知分子生成的TamGen。

当这些模型雨点般出现时，我们不应放弃科学的堡垒。相反，是时候要求AI与机器学习遵循**最高标准的科学探索**。我们需要把重点放在可重复性上，更重要的是强调提供**机制洞见**与**理解**的理论概念与方法。

AI无疑能给科学带来巨大益处，但我们绝不可背离三百年来经受考验的**理性与经验的可重复融合**。一条可行路径是**可解释AI**，另一条是我们应拥抱**因果AI**；前提是AI能够以科学术语解释其内部机理与预测。第三条路径是**Big AI**，即将机器学习与基于物理的方法结合，使AI受**自然规律约束**。在这些语境下，二者的优缺点相辅相成，在药物发现等任务中结合更可能奏效。

科学是人类最珍贵的创造之一，比以往任何时候都更需要**捍卫与阐明**。培根的蜜蜂正受到AI的威胁，而它们需要繁盛。**AI必须遵循科学方法**。

> 小编锐评：
> - AI当然非常有用，我们天天都在高强度使用。但我讨厌的是追逐风口就能盈利这种环境，某些“宣讲/本子不带AI就会被拒”的现象之下，是舍本逐末，是人类的非理性。
> - 至于在科学领域上的应用，需要明确地定义该模型的使用范围，严格地遵守规范（如OECD Principles）。做科学最终是要回到逻辑上的，也许真正的可解释性不存在或只能从数学上理解，那它们也永远是做engineering的工具或人类的智能助手。

